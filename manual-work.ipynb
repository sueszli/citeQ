{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs = [\n",
    "    \"Yousra Aafer\",\n",
    "    \"Samer Al-Kiswany\",\n",
    "    \"N. Asokan\",\n",
    "    \"Sepehr Assadi\",\n",
    "    \"Joanne Atlee\",\n",
    "    \"Gladimir Baranoski\",\n",
    "    \"Diogo Barradas\",\n",
    "    \"Christopher Batty\",\n",
    "    \"Shai Ben-David\",\n",
    "    \"Shalev Ben-David\",\n",
    "    \"Daniel Berry\",\n",
    "    \"Therese Biedl\",\n",
    "    \"Eric Blais\",\n",
    "    \"Raouf Boutaba\",\n",
    "    \"Yuri Boykov\",\n",
    "    \"Tim Brecht\",\n",
    "    \"Dan Brown\",\n",
    "    \"Trevor Brown\",\n",
    "    \"Peter Buhr\",\n",
    "    \"Wenhu Chen\",\n",
    "    \"Charles Clarke\",\n",
    "    \"Richard Cleve\",\n",
    "    \"Robin Cohen\",\n",
    "    \"Gordon Cormack\",\n",
    "    \"Khuzaima Daudjee\",\n",
    "    \"Nancy Day\",\n",
    "    \"Yuntian Deng\",\n",
    "    \"Kimon Fountoulakis\",\n",
    "    \"Mark Giesbrecht\",\n",
    "    \"Michael Godfrey\",\n",
    "    \"Ian Goldberg\",\n",
    "    \"Sergey Gorbunov\",\n",
    "    \"Maura R. Grossman\",\n",
    "    \"Toshiya Hachisuka\",\n",
    "    \"Mohammad Hajiabadi\",\n",
    "    \"Jason Hartford\",\n",
    "    \"Xi He\",\n",
    "    \"Urs Hengartner\",\n",
    "    \"Jesse Hoey\",\n",
    "    \"Xiao Hu\",\n",
    "    \"Ihab F. Ilyas\",\n",
    "    \"Gautam Kamath\",\n",
    "    \"Craig S. Kaplan\",\n",
    "    \"Lila Kari\",\n",
    "    \"Martin Karsten\",\n",
    "    \"Florian Kerschbaum\",\n",
    "    \"George Labahn\",\n",
    "    \"Kate Larson\",\n",
    "    \"Lap Chi Lau\",\n",
    "    \"Edith Law\",\n",
    "    \"Ondřej Lhoták\",\n",
    "    \"Ming Li\",\n",
    "    \"Yuying Li\",\n",
    "    \"Noura Limam\",\n",
    "    \"Jimmy Lin\",\n",
    "    \"Sihang Liu\",\n",
    "    \"Yang Lu\",\n",
    "    \"Bin Ma\",\n",
    "    \"Sujaya Maiyya\",\n",
    "    \"Richard Mann\",\n",
    "    \"Stephen Mann\",\n",
    "    \"Ali José Mashtizadeh\",\n",
    "    \"Shane McIntosh\",\n",
    "    \"Meiyappan (Mei) Nagappan\",\n",
    "    \"Pengyu Nie\",\n",
    "    \"Naomi Nishimura\",\n",
    "    \"Rafael Oliveira\",\n",
    "    \"Jeff Orchard\",\n",
    "    \"Tamer Özsu\",\n",
    "    \"Pascal Poupart\",\n",
    "    \"Prabhakar Ragde\",\n",
    "    \"Mohammad Salahuddin\",\n",
    "    \"Ken Salem\",\n",
    "    \"Semih Salihoğlu\",\n",
    "    \"Éric Schost\",\n",
    "    \"Jeffrey Shallit\",\n",
    "    \"Freda Shi\",\n",
    "    \"Shlomi Steinberg\",\n",
    "    \"Arne Storjohann\",\n",
    "    \"Chengnian Sun\",\n",
    "    \"Mina Tahmasbi Arashloo\",\n",
    "    \"David Toman\",\n",
    "    \"Richard Trefler\",\n",
    "    \"Olga Veksler\",\n",
    "    \"Daniel Vogel\",\n",
    "    \"Justin Wan\",\n",
    "    \"Stephen Watt\",\n",
    "    \"Grant Weddell\",\n",
    "    \"Bernard Wong\",\n",
    "    \"Meng Xu\",\n",
    "    \"Yaoliang Yu\",\n",
    "    \"Hong Zhang\",\n",
    "    \"Hongyang Zhang\",\n",
    "    \"Yizhou Zhang\",\n",
    "    \"Jian Zhao\",\n",
    "    \"Victor Zhong\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Batty,144366758\n",
      "Wenhu Chen,2928777\n",
      "Tamer Özsu,1705151\n",
      "Hongyang Zhang,40975176\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#write two clumn csv file\n",
    "with open('profs1.csv', 'w') as f:\n",
    "    for prof in profs:\n",
    "        res = requests.get('https://api.semanticscholar.org/graph/v1/author/search?query=' + prof.replace(' ', '+').lower() + '&fields=name,aliases,affiliations').json()\n",
    "        if res['total'] == 1:\n",
    "            f.write(prof + ',' + res['data'][0]['authorId'] + '\\n')\n",
    "            continue\n",
    "        aff = False\n",
    "        for prof_d in res['data']:\n",
    "            if \"University of Waterloo\" in prof_d['affiliations']:\n",
    "                f.write(prof + ',' + prof_d['authorId'] + '\\n')\n",
    "                print(prof + ',' + prof_d['authorId'])\n",
    "                aff = True\n",
    "                break\n",
    "        if not aff:\n",
    "            f.write(prof + ',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onurerenarpaci/Desktop/citeQ/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"citations_annotated.csv\", \"r\") as f:\n",
    "    citations = f.readlines()\n",
    "    citations = [c.strip().split(\",\") for c in citations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.875, 0.7142857142857143, 0.4305555555555556, 0.9230769230769231]\n",
      "Recall: [0.1891891891891892, 0.625, 0.96875, 0.5217391304347826]\n"
     ]
    }
   ],
   "source": [
    "with open(\"llm_annotations_gpt4_3class.csv\", \"r\") as f:\n",
    "    llm_annotations = f.readlines()\n",
    "    llm_annotations = [c.strip().split(\",\") for c in llm_annotations]\n",
    "    llm_annotations = [[c[0], int(c[1]), int(c[2])] for c in llm_annotations]\n",
    "\n",
    "# calculate precision, and recall for each class\n",
    "tp = [0, 0, 0, 0]\n",
    "fp = [0, 0, 0, 0]\n",
    "fn = [0, 0, 0, 0]\n",
    "\n",
    "for annotation in llm_annotations:\n",
    "    if annotation[1] == annotation[2]:\n",
    "        tp[annotation[1]] += 1\n",
    "    else:\n",
    "        fp[annotation[2]] += 1\n",
    "        fn[annotation[1]] += 1\n",
    "\n",
    "precision = [0, 0, 0, 0]\n",
    "recall = [0, 0, 0, 0]\n",
    "for i in range(4):\n",
    "    precision[i] = tp[i] / (tp[i] + fp[i]) if tp[i] + fp[i] != 0 else 0\n",
    "    recall[i] = tp[i] / (tp[i] + fn[i])\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlmClassifier.get_sentiment_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Hello, how are you? [/INST]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 8, 32, 23]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"citations_annotated.csv\", \"r\") as f:\n",
    "    citations = f.readlines()\n",
    "    citations = [c.strip().split(\",\") for c in citations]\n",
    "\n",
    "class_count = [0, 0, 0, 0]\n",
    "for citation in citations:\n",
    "    class_count[int(citation[1])] += 1\n",
    "\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37, 0.08, 0.32, 0.23]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = [x/100 for x in class_count]\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd', 'iejirgr', 'sdklsjei', 'skefekl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['asd', 'sdklsjei', 'iejirgr', 'skefekl']\n",
    "a = sorted(a)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
